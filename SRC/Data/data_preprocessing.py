# -*- coding: utf-8 -*-
"""Data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vY4HwNI5Fb2lM1NuZb2uAepYXRnnutu-
"""

import os
import pickle
import urllib.request

import numpy as np
import pandas as pd


# ============================================================
# CONFIGURACIÓN: elegir dataset
# ============================================================

# Cambiar a 'goodreads' si se quiere trabajar con libros
DATASET = "netflix"   # 'netflix' o 'goodreads'


# ============================================================
# Descarga de datos desde el repo oficial si no están presentes
# ============================================================

BASE_URL = (
    "https://raw.githubusercontent.com/"
    "DiploDatos/AprendizajePorRefuerzos/master/"
    "tp_decision_transformer/data"
)

# Archivos disponibles en el repo oficial
TRAIN_FILES = [
    "train/netflix8_train.df",
    "train/netflixM8_train.df",
    "train/goodreads8_train.df",
]

TEST_FILES = [
    "test_users/netflix8_test.json",
    "test_users/goodreads8_test.json",
]

GROUP_FILES = [
    "groups/mu_goodreads8.csv",
    "groups/mu_netflix8.csv",
    "groups/mu_netflixM8.csv",
]


def download_if_missing(relative_path: str) -> None:
    """
    Descarga un archivo de datos desde GitHub si no existe localmente.

    relative_path: ruta relativa dentro de la carpeta data/
                   (por ejemplo 'train/netflix8_train.df')
    """
    local_path = os.path.join("data", relative_path)

    if os.path.exists(local_path):
        return  # archivo ya presente

    url = f"{BASE_URL}/{relative_path}"
    os.makedirs(os.path.dirname(local_path), exist_ok=True)

    print(f"Descargando {relative_path} desde {url}")
    urllib.request.urlretrieve(url, local_path)
    print(f"Archivo guardado en {local_path}")


def ensure_all_data_files():
    """
    Asegura que los archivos de datos relevantes existan en data/.
    Descarga desde el repo de DiploDatos si faltan.
    """
    for rel in TRAIN_FILES + TEST_FILES + GROUP_FILES:
        download_if_missing(rel)

    print("Archivos de datos verificados/descargados.")


def get_paths_for_dataset(dataset: str):
    """
    Devuelve los paths relativos para train/test/groups
    según el dataset elegido.
    """
    if dataset == "netflix":
        train_rel = "train/netflix8_train.df"
        test_rel = "test_users/netflix8_test.json"
        group_rel = "groups/mu_netflix8.csv"
    elif dataset == "goodreads":
        train_rel = "train/goodreads8_train.df"
        test_rel = "test_users/goodreads8_test.json"
        group_rel = "groups/mu_goodreads8.csv"
    else:
        raise ValueError("DATASET debe ser 'netflix' o 'goodreads'")

    return (
        os.path.join("data", train_rel),
        os.path.join("data", test_rel),
        os.path.join("data", group_rel),
    )


# ============================================================
# Preprocesamiento: formato Decision Transformer
# ============================================================

def create_dt_dataset(df_train: pd.DataFrame):
    """
    Conversión del dataset raw al formato esperado por el Decision Transformer.

    Args:
        df_train: DataFrame con columnas [user_id, user_group, items, ratings]

    Returns:
        trajectories: lista de diccionarios, uno por usuario, con las claves:
            - 'items': numpy array de item IDs
            - 'ratings': numpy array de ratings
            - 'returns_to_go': numpy array con suma de rewards futuros
            - 'timesteps': numpy array con índices temporales [0, 1, ..., T-1]
            - 'user_group': entero con el grupo del usuario
    """
    trajectories = []

    for _, row in df_train.iterrows():
        items = np.asarray(row["items"], dtype=np.int64)
        ratings = np.asarray(row["ratings"], dtype=np.float32)
        group = int(row["user_group"])

        # Cálculo de returns-to-go (R̂_t = r_t + R̂_{t+1})
        returns_to_go = np.zeros_like(ratings, dtype=np.float32)
        returns_to_go[-1] = ratings[-1]
        for t in range(len(ratings) - 2, -1, -1):
            returns_to_go[t] = ratings[t] + returns_to_go[t + 1]

        timesteps = np.arange(len(items), dtype=np.int64)

        trajectory = {
            "items": items,
            "ratings": ratings,
            "returns_to_go": returns_to_go,
            "timesteps": timesteps,
            "user_group": group,
        }
        trajectories.append(trajectory)

    return trajectories


def main():
    # 1) Verificación/descarga de datos
    ensure_all_data_files()

    # 2) Paths según el DATASET seleccionado
    train_path, test_path, group_path = get_paths_for_dataset(DATASET)

    print(f"Usando DATASET = {DATASET}")
    print(f"Cargando datos de entrenamiento desde: {train_path}")
    df_train = pd.read_pickle(train_path)

    # 3) Creación de trayectorias en formato Decision Transformer
    trajectories = create_dt_dataset(df_train)

    # 4) Guardado en data/processed/
    processed_dir = "data/processed"
    os.makedirs(processed_dir, exist_ok=True)

    output_path = os.path.join(processed_dir, f"{DATASET}_trajectories_train.pkl")
    with open(output_path, "wb") as f:
        pickle.dump(trajectories, f)

    # 5) Estadísticas básicas para logging
    lengths = np.array([len(t["items"]) for t in trajectories])
    initial_returns = np.array([t["returns_to_go"][0] for t in trajectories])

    print("\n=== Resumen de preprocesamiento ===")
    print(f"Dataset: {DATASET}")
    print(f"Cantidad de trayectorias: {len(trajectories)}")
    print(f"Longitud promedio: {lengths.mean():.2f}")
    print(f"Longitud min / max: {lengths.min()} / {lengths.max()}")
    print(f"R̂_0 promedio: {initial_returns.mean():.2f}")
    print(f"Archivo guardado en: {output_path}")


if __name__ == "__main__":
    main()